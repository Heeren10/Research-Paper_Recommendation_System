{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62b84e9-343d-4ef7-aaf8-c67d680a51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harshhal\\OneDrive\\Desktop\\Python\\tf-env\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.11) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "# is used for safely evaluating strings containing Python literals or container displays\n",
    "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92bcbb75-1887-4695-aba6-4709ec73a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv(\"arxiv_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcea371f-f85c-4961-b6a4-96d98edb8020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>Stereo matching is one of the widely used tech...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>Consistency training has proven to be an advan...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>To ensure safety in automated driving, the cor...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Stereo matching is one of the widely used tech...   \n",
       "1  The recent advancements in artificial intellig...   \n",
       "2  In this paper, we proposed a novel mutual cons...   \n",
       "3  Consistency training has proven to be an advan...   \n",
       "4  To ensure safety in automated driving, the cor...   \n",
       "\n",
       "                         terms  \n",
       "0           ['cs.CV', 'cs.LG']  \n",
       "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
       "2           ['cs.CV', 'cs.AI']  \n",
       "3                    ['cs.CV']  \n",
       "4           ['cs.CV', 'cs.LG']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c2895a-1b4b-4bba-80d4-8cb3d1b4f670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51774, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c601a18-85c7-4577-836a-fdb7070d1b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titles       0\n",
       "summaries    0\n",
       "terms        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b3a1c7-59df-4aab-9716-67a2ebb78378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(12783)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fba494f-637e-4fa1-beba-183ce18eae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : ['cs.CV' 'cs.LG' 'cs.AI' ... 'I.2.6; I.5.1; G.3'\n",
      " '92E10, 46M20, 94A08, 68U10, 44A12, 55R35' '92E10']\n",
      "lenght : 1099\n"
     ]
    }
   ],
   "source": [
    "# getting unique labels\n",
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()\n",
    "print(\"labels :\",labels)\n",
    "print(\"lenght :\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b8d986-acca-4a83-882a-1fbc549c1c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 38972 rows in the deduplicated dataset.\n",
      "2321\n",
      "3157\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate entries based on the \"titles\" (terms) column\n",
    "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
    "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
    "# There are some terms with occurrence as low as 1.\n",
    "print(sum(arxiv_data['terms'].value_counts()==1))\n",
    "# how many unique terms\n",
    "print(arxiv_data['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc298dd-245c-4e4f-9db1-68970e680101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36651, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6996ee9a-2d53-469b-95a1-bd5a6204c7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.CV', 'cs.LG']), list(['cs.CV', 'cs.AI', 'cs.LG']),\n",
       "       list(['cs.CV', 'cs.AI'])], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88647b57-061a-4470-b6cb-ef8df08dd46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 32985\n",
      "Number of rows in validation set: 1833\n",
      "Number of rows in test set: 1833\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
    "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a390a49f-5476-44a6-9c81-20217ab49e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', np.str_('cs.CV'), np.str_('cs.LG'), np.str_('stat.ML'), np.str_('cs.AI'), np.str_('eess.IV'), np.str_('cs.RO'), np.str_('cs.CL'), np.str_('cs.NE'), np.str_('cs.CR'), np.str_('math.OC'), np.str_('eess.SP'), np.str_('cs.GR'), np.str_('cs.SI'), np.str_('cs.MM'), np.str_('cs.SY'), np.str_('cs.IR'), np.str_('cs.MA'), np.str_('eess.SY'), np.str_('cs.HC'), np.str_('math.IT'), np.str_('cs.IT'), np.str_('cs.DC'), np.str_('cs.CY'), np.str_('stat.AP'), np.str_('stat.TH'), np.str_('math.ST'), np.str_('stat.ME'), np.str_('eess.AS'), np.str_('cs.SD'), np.str_('q-bio.QM'), np.str_('q-bio.NC'), np.str_('cs.DS'), np.str_('cs.GT'), np.str_('cs.NI'), np.str_('cs.SE'), np.str_('cs.CG'), np.str_('I.2.6'), np.str_('stat.CO'), np.str_('math.NA'), np.str_('cs.NA'), np.str_('physics.chem-ph'), np.str_('cs.DB'), np.str_('q-bio.BM'), np.str_('cs.LO'), np.str_('cs.PL'), np.str_('cond-mat.dis-nn'), np.str_('68T45'), np.str_('math.PR'), np.str_('physics.comp-ph'), np.str_('cs.CE'), np.str_('cs.AR'), np.str_('I.2.10'), np.str_('q-fin.ST'), np.str_('cond-mat.stat-mech'), np.str_('quant-ph'), np.str_('math.DS'), np.str_('68T05'), np.str_('physics.data-an'), np.str_('cs.CC'), np.str_('I.4.6'), np.str_('physics.soc-ph'), np.str_('physics.ao-ph'), np.str_('q-bio.GN'), np.str_('econ.EM'), np.str_('cs.DM'), np.str_('physics.med-ph'), np.str_('cs.PF'), np.str_('astro-ph.IM'), np.str_('I.4.8'), np.str_('math.AT'), np.str_('cs.FL'), np.str_('I.4'), np.str_('q-fin.TR'), np.str_('I.5.4'), np.str_('I.2'), np.str_('68U10'), np.str_('hep-ex'), np.str_('cond-mat.mtrl-sci'), np.str_('68T10'), np.str_('physics.optics'), np.str_('physics.geo-ph'), np.str_('physics.flu-dyn'), np.str_('math.AP'), np.str_('I.4; I.5'), np.str_('I.4.9'), np.str_('I.2.6; I.2.8'), np.str_('68T01'), np.str_('65D19'), np.str_('q-fin.CP'), np.str_('nlin.CD'), np.str_('math.CO'), np.str_('cs.MS'), np.str_('I.2.6; I.5.1'), np.str_('I.2.10; I.4; I.5'), np.str_('I.2.0; I.2.6'), np.str_('68T07'), np.str_('cs.SC'), np.str_('cs.ET'), np.str_('K.3.2'), np.str_('I.2.8'), np.str_('I.2.10; I.4.8'), np.str_('68U01'), np.str_('68T30'), np.str_('q-fin.GN'), np.str_('q-fin.EC'), np.str_('q-bio.MN'), np.str_('econ.GN'), np.str_('I.4.9; I.5.4'), np.str_('I.4.5'), np.str_('I.2; I.5'), np.str_('I.2; I.4; I.5'), np.str_('I.2.6; I.2.7'), np.str_('68T99'), np.str_('68Q32'), np.str_('68'), np.str_('62H30'), np.str_('q-fin.RM'), np.str_('q-fin.PM'), np.str_('q-bio.TO'), np.str_('q-bio.OT'), np.str_('physics.bio-ph'), np.str_('nlin.AO'), np.str_('math.LO'), np.str_('math.FA'), np.str_('hep-ph'), np.str_('cond-mat.soft'), np.str_('I.4.6; I.4.8'), np.str_('I.4.4'), np.str_('I.4.3'), np.str_('I.4.0'), np.str_('I.2; J.2'), np.str_('I.2; I.2.6; I.2.7'), np.str_('I.2.7'), np.str_('I.2.6; I.5.4'), np.str_('I.2.6; I.2.9'), np.str_('I.2.6; I.2.7; H.3.1; H.3.3'), np.str_('I.2.6; I.2.10'), np.str_('I.2.6, I.5.4'), np.str_('I.2.1; J.3'), np.str_('I.2.10; I.5.1; I.4.8'), np.str_('I.2.10; I.4.8; I.5.4'), np.str_('I.2.10; I.2.6'), np.str_('I.2.1'), np.str_('H.3.1; I.2.6; I.2.7'), np.str_('H.3.1; H.3.3; I.2.6; I.2.7'), np.str_('G.3'), np.str_('F.2.2; I.2.7'), np.str_('E.5; E.4; E.2; H.1.1; F.1.1; F.1.3'), np.str_('68Txx'), np.str_('62H99'), np.str_('62H35'), np.str_('14J60 (Primary) 14F05, 14J26 (Secondary)')]\n"
     ]
    }
   ],
   "source": [
    "# creates a TensorFlow RaggedTensor (terms) from the values in the \"terms\" column of the train_df DataFrame. A RaggedTensor is a tensor with non-uniform shapes\n",
    "terms = tf.ragged.constant(train_df['terms'].values)\n",
    "# This line creates a StringLookup layer in TensorFlow. The purpose of this layer is to map strings to integer indices and vice versa. The output_mode=\"multi_hot\" indicates that the layer will output a multi-hot encoded representation of the input strings.\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "# This step adapts the StringLookup layer to the unique values in the \"terms\" column, building the vocabulary.\n",
    "lookup.adapt(terms)\n",
    "# retrieve vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a90705e-44c4-4408-a229-10ca40eb165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['cs.LG']\n",
      "Label-binarized representation: [[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e991c16-ecd2-4b5c-9564-948ad9902a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_multi_hot(encoded_vector):\n",
    "    \"\"\"\n",
    "    Converts a multi-hot encoded vector back to its corresponding labels.\n",
    "    \n",
    "    Args:\n",
    "        encoded_vector (array-like): Multi-hot encoded label vector\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Original label names\n",
    "    \"\"\"\n",
    "    return [\n",
    "        str(vocab[i])\n",
    "        for i, value in enumerate(encoded_vector)\n",
    "        if value == 1 and vocab[i] != \"[UNK]\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee8d79ef-edd9-464b-8ae2-d7e828b7e38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn summary, the make_dataset function is designed to create a \\ndataset suitable for training a model. It takes a dataframe as input, \\nassumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of \\nbatches where each batch consists of abstract \\nsequences and their corresponding binarized label sequences. \\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# following lines::\n",
    "# which is used for automatic adjustment of resource usage by TensorFlow's data loading pipeline.\n",
    "\n",
    "#max_seqlen: Maximum sequence length. It indicates the maximum length allowed for sequences.\n",
    "max_seqlen = 150\n",
    "#batch_size: Batch size. It specifies the number of samples to use in each iteration.\n",
    "batch_size = 128\n",
    "#padding_token: A token used for padding sequences.\n",
    "padding_token = \"<pad>\"\n",
    "#auto = tf.data.AUTOTUNE: auto is assigned the value tf.data.AUTOTUNE,\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # creating sequences of labesls\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    #This line uses the previously defined lookup layer to convert the ragged tensor of labels into a binarized representation. The resulting label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    # creating sequences of text.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"summaries\"].values, label_binarized))\n",
    "    # shuffling data basis on condition\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "In summary, the make_dataset function is designed to create a \n",
    "dataset suitable for training a model. It takes a dataframe as input, \n",
    "assumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of \n",
    "batches where each batch consists of abstract \n",
    "sequences and their corresponding binarized label sequences. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a424a-c87d-4480-b6f1-e0cd9773fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "842c4bb3-436f-4d87-8925-9a0057a3d509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: b'Natural Language Search (NLS) extends the capabilities of search engines that\\nperform keyword search allowing users to issue queries in a more \"natural\"\\nlanguage. The engine tries to understand the meaning of the queries and to map\\nthe query words to the symbols it supports like Persons, Organizations, Time\\nExpressions etc.. It, then, retrieves the information that satisfies the user\\'s\\nneed in different forms like an answer, a record or a list of records. We\\npresent an NLS system we implemented as part of the Search service of a major\\nCRM platform. The system is currently in production serving thousands of\\ncustomers. Our user studies showed that creating dynamic reports with NLS saved\\nmore than 50% of our user\\'s time compared to achieving the same result with\\nnavigational search. We describe the architecture of the system, the\\nparticularities of the CRM domain as well as how they have influenced our\\ndesign decisions. Among several submodules of the system we detail the role of\\na Deep Learning Named Entity Recognizer. The paper concludes with discussion\\nover the lessons learned while developing this product.'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m label \u001b[38;5;241m=\u001b[39m label_batch[i]\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbstract: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel(s):\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43minvert_multi_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 11\u001b[0m, in \u001b[0;36minvert_multi_hot\u001b[1;34m(encoded_vector)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvert_multi_hot\u001b[39m(encoded_vector):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Converts a multi-hot encoded vector back to its corresponding labels.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m        List[str]: Original label names\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mstr\u001b[39m(vocab[i])\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(encoded_vector)\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m vocab[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[51], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvert_multi_hot\u001b[39m(encoded_vector):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Converts a multi-hot encoded vector back to its corresponding labels.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m        List[str]: Original label names\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mstr\u001b[39m(vocab[i])\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(encoded_vector)\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m \u001b[38;5;129;01mand\u001b[39;00m vocab[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m     ]\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# This code snippet is iterating through batches of the training dataset and printing the abstract text along with the corresponding labels.\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(\"Label(s):\", \", \".join(invert_multi_hot(label)))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab02cd47-0dbd-44d4-8403-0c8c287bff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153537\n"
     ]
    }
   ],
   "source": [
    "# This code calculates the size of the vocabulary in the \"abstracts\" column of the train_df DataFrame.\n",
    "\n",
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"summaries\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b1a2f-975f-4f97-9d7d-b9e01f583e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
